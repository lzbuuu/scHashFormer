# scHashFormer
The code of Fast and Scalable scRNA-seq Clustering via Local Attention.
In this paper, we present \textbf{scHashFormer}, which replaces all-pairs computation with \emph{learnable locality-sensitive hashing} to group transcriptionally similar cells into high-purity hash neighbourhoods in near-linear time. Within each neighbourhood, a lightweight Transformer performs message passing to learn structure-aware embeddings. Training is regularised by a zero-inflated negative binomial (ZINB) objective, removing the need for an explicit global graph. Across datasets spanning 1e4-1e6 cells, scHashFormer outperforms leading methods while reducing runtime and memory by up to an order of magnitude and avoiding out-of-memory failures. The resulting embeddings improve cell-type annotation, better separate rare and transitional populations, enable batch-robust integration without over-mixing, and preserve developmental trajectories consistent with lineage markers. Together, scHashFormer makes million-cell clustering and downstream single-cell analyses routine and reproducible at the atlas scale.

# Architecture
The neural network architecture of scHashFormer. LSH indexer: a multi-head, learnable LSH encoder produces continuous hash embeddings and binary hash codes; identical codes define hash neighbourhoods. A contrastive objective tightens within-neighbourhood similarity and separates cross-neighbourhood pairs, and a ZINB decoder reconstructs counts to retain count-based biology. Affinity encoder (attention-based): for each cell, \emph{tokenised same-neighbourhood cells} are encoded by a lightweight neighbourhood Transformer to obtain structure-aware embeddings; a KL-based head yields soft cluster assignments, and ZINB reconstruction regularises the embedding. The workflow preserves local manifold structure while avoiding the construction of all pairwise relationships.![fram1 (1)](./scHashFormer.png)
